{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "----------------------- 2 - Text pre-processing ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19075, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP = pd.read_csv('Desktop/test/FP2.csv')\n",
    "FP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = FP.replace(r'^\\s+$', np.nan, regex=True) #remplacer espaces vides par des NaN\n",
    "# FP.isnull().sum() # nombre de NaN par colonne de la DataBase\n",
    "FP = FP[FP['name'].notna()] # Delete NaN in 'name' columns\n",
    "FP = FP.reset_index(drop=True)\n",
    "FP = FP[['rcn', 'name', 'role','country', 'title', 'objective','startDate', 'endDate']]# conserver les seules colonnes qui vous intéresent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP['namekey'] = FP['name'] #dupliquer colonne avec données\n",
    "FP.loc[:,\"namekey\"] = FP.namekey.apply(lambda x : str.lower(x)) #Convert text to lowercase\n",
    "FP.loc[:,\"namekey\"] = FP.namekey.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')# remove accent\n",
    "FP.loc[:,\"namekey\"] = FP.namekey.str.strip() # Remove whitespaces (at the beginning and at the end of each row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #Remove numbers\n",
    "FP.loc[:,\"namekey\"] = FP.namekey.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
    "FP.loc[:,\"namekey\"] = FP.namekey.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #tokenize namekey\n",
    "from nltk.tokenize import word_tokenize\n",
    "FP['namekey'] = FP.apply(lambda row: nltk.word_tokenize(row['namekey']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer #Stemming is the process of reducing words to their stem\n",
    "stemmer = SnowballStemmer(language=\"english\") # Use English stemmer. #utiliser d'autres langues ?\n",
    "FP['namekey'] = FP['namekey'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer1 = SnowballStemmer(language=\"french\") # Use English stemmer. #utiliser d'autres langues ?\n",
    "FP['namekey'] = FP['namekey'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer2 = SnowballStemmer(language=\"german\") # Use English stemmer. #utiliser d'autres langues ?\n",
    "FP['namekey'] = FP['namekey'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer3 = SnowballStemmer(language=\"italian\") # Use English stemmer. #utiliser d'autres langues ?\n",
    "FP['namekey'] = FP['namekey'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/leo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords') #Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_list = (stopwords.words('english') +\n",
    "                        stopwords.words('french')+ \n",
    "                        stopwords.words('german')+ \n",
    "                        stopwords.words('italian')+ \n",
    "                        stopwords.words('greek')+\n",
    "                        stopwords.words('danish')+\n",
    "                        stopwords.words('dutch')+\n",
    "                        stopwords.words('finnish')+\n",
    "                        stopwords.words('portuguese')+\n",
    "                        stopwords.words('spanish'))\n",
    "def remove_stopWords(s):\n",
    "    '''For removing stop words\n",
    "    '''\n",
    "    s = ' '.join(word for word in s.split() if word not in stopwords_list)\n",
    "    return s\n",
    "FP.loc[:,\"namekey\"] = FP.namekey.apply(lambda x : [remove_stopWords(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/leo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/leo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # Lemmatizing\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence)) #tokenize the sentence and find the POS tag for each token\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged) #tuple of (token, wordnet_tag)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word) #if there is no available tag, append the token as is\n",
    "        else:\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag)) #else use the tag to lemmatize the token\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "FP.loc[:,\"namekey\"] = FP['namekey'].apply(lambda x: [lemmatize_sentence(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer #detokenise \"namekey\" column\n",
    "FP['namekey']=FP['namekey'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(FP.loc[:,\"namekey\"].isna().sum()) #last check there is non NaN in the \"namekey\" column\n",
    "FP.loc[FP['namekey'].isna(),'namekey'] = FP.loc[FP['namekey'].isna(),'name'] # if \"namekey\"=NaN après pre-processing, remplacer par 'name' correspondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pandas(l): #\"Harmoniser classement interne des cases\"\n",
    "    l.sort()\n",
    "    return l\n",
    "FP.loc[:,\"namekey\"] = FP.namekey.apply(lambda x : \" \".join(sort_pandas(x.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP.to_csv(\"Desktop/test/FP_pre_txt_process.csv\",mode = 'w', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
